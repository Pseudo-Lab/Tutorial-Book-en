{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSOi1ls_QSmw"
      },
      "source": [
        "# 1. Introduction to Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0UVWsRozjAg"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pseudo-Lab/Tutorial-Book/blob/master/book/chapters/object-detection/Ch1-Object-Detection.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "executionInfo": {
          "elapsed": 810,
          "status": "ok",
          "timestamp": 1607606734959,
          "user": {
            "displayName": "안성진",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCjgkN_MvtrSUHRuFvstrWm6fhi5cf7CKd2UHYAw=s64",
            "userId": "00266029492778998652"
          },
          "user_tz": -540
        },
        "id": "GYWLo2QQqbWw",
        "outputId": "7fbb8fe5-7b61-48f6-c08f-bbcfa90801fa",
        "tags": [
          "remove-input"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JOzDr2eGFcM\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 10,
          "metadata": {
            "tags": [

            ]
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/JOzDr2eGFcM\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSYHbN-UpvJm"
      },
      "source": [
        "Object Detection is a field of computer vision technology in which objects of interest within a given image are detected.\n",
        "\n",
        "If an artificial intelligence model determines that the image on the left in Figure 1-1 is of a dog, the model is an image classification model. However, if the artificial intelligence model classifies the object as a dog while also detecting the location of the object, as shown in the picture on the right, the model is an object detection model.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1qFN7jJpM-VsR7SIMvbxDO4fGrvqCZvrz)\n",
        "\n",
        "- 그림 1-1 이미지 분류 모델과 객체 탐지 모델 비교 (출처: https://www.pexels.com/search/dog/)\n",
        "\n",
        "The object detection model can be used in many fields. The most representative case is in an autonomous vehicle. In order to create an autonomous vehicle, computers must be able to recognize surrounding objects by themselves. For instance, the computer should recognize traffic signals; when there is a red light, the vehicle should know to stop.\n",
        "\n",
        "Object detection technology is also used for efficient resource management in the field of security. In general, CCTVs record continuously, so a huge amount of memory is required. However, in combination with object detection technology, memory can be used efficiently by starting a recording only when a specific object is detected.\n",
        "\n",
        "In this chapter, we will build an object detection model that detects masks. The model we built detects the position of the face in a given image and checks if the face is masked. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws49__laCHZL"
      },
      "source": [
        "## 1.1. Bounding Box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhCfyToEDg90"
      },
      "source": [
        "Before creating an object detection model, the first step is to create a bounding box. Since the amount of data used in the object detection model is vast, the object can be correctly detected through a bounding box. In the deep learning process, only the bounding box area is targeted, so we can train efficiently.\n",
        "\n",
        "The bounding box is a method that helps us train a model efficiently by detecting specific objects. In the object detection model, bounding boxes are used to specify the target location. The target position is expressed as a rectangle using the X and Y axes. The bounding box value is expressed as (X min, Y min, X max, Y max).\n",
        "\n",
        "![](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/raw/master/img/bc1.PNG)\n",
        "\n",
        "- 그림 1-2 바운딩 영역 픽셀값으로 지정 (출처: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection)\n",
        "\n",
        "As shown in Figure 1-2, the area between the minimum and maximum X and Y values is set as the bounding box area. However, the X and Y values in Figure 1-2 are pixel values and should be converted into a value between 0 and 1 for efficient calculation.\n",
        "\n",
        "![](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/raw/master/img/bc2.PNG)\n",
        "\n",
        "- 그림 1-3 바운딩 영역 백분위로 지정 (출처: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/raw/master/img/bc2.PNG)\n",
        "\n",
        "The X and Y values in Figure 1-3 are calculated by dividing the original pixel X- and Y-values of the bounding box by the maximum pixel X-value of 971 and the maximum pixel Y-value of 547, respectively. For example, the bounding box minimum X-value of 640 is divided by 971 to get 0.66. This normalization can be seen as a process for efficient computation, but it is not essential.\n",
        "\n",
        "Depending on the dataset, the bounding box value may be included as metadata. If there is no metadata, the bounding box can be specified through separate code implementation. The [Face Mask Detection](https://www.kaggle.com/andrewmvd/face-mask-detection) dataset used in this tutorial provides bounding boxes. We will diagram the bounding box in chapter 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGyRmZ_OmTg7"
      },
      "source": [
        "## 1.2. Model Type\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47j_MmGnp0OJ"
      },
      "source": [
        "The object detection model can be largely divided into one-stage and two-stage models. Let's take a look at each model type.\n",
        "\n",
        "![](https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/pics/ch1img04.PNG?raw=true)\n",
        "\n",
        "- 그림 1-4 객체 탐지 알고리즘 타임라인 (출처: Zou et al. 2019. Object Detection in 20 Years: A Survey)\n",
        "\n",
        "Figure 1-4 shows the genealogy of the object detection model. Object detection models based on deep learning appeared in 2012 and can be divided into one-stage detectors and two-stage detectors. To understand the two types of flows, we need to understand the concepts of classification and region proposal. Classification is to classify an object, and region proposal is an algorithm that finds an area where an object is likely to be.\n",
        "\n",
        "Two-stage detectors perform well in terms of object detection accuracy, but are limited to real-time detection due to their slow prediction speed. To solve this speed problem, one-stage detectors, which simultaneously perform classification and region proposition, have been created. In the next section, we will examine the structure of one-stage and two-stage detectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytw-fepWIQeN"
      },
      "source": [
        "### 1.2.1. One-Stage Detector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMUkvx5Op5pU"
      },
      "source": [
        "One-stage detection is a method of obtaining results by performing classification and regional proposal simultaneously. After inputting the image into the model, image features are extracted using the Conv Layer as shown in Figure 1-5.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1850eKsb59NtgQEcM0fXji7cD13TEsDo3)\n",
        "\n",
        "- 그림 1-5 One-Stage Detector 구조(출처:https://jdselectron.tistory.com/101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOTv6Y0tIT0p"
      },
      "source": [
        "### 1.2.2. Two-Stage Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnZGsB22p8hL"
      },
      "source": [
        "Two-stage detection is a method of obtaining results by sequentially performing classification and regional proposal. As shown in Figure 1-6, we can see that region proposal and classification are executed sequentially.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1qACO-vEahSiz2Zb5jmAW1jbw94g1ThZO)\n",
        "\n",
        "- 그림 1-6 Two-Stage Detector 구조(출처:https://jdselectron.tistory.com/101)\n",
        "\n",
        "As a result, one-stage detection is relatively fast but has low accuracy, and two-stage detection is relatively slow but has higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLoukyG4Ca7l"
      },
      "source": [
        "## 1.3. Model Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk68eNE-qNKq"
      },
      "source": [
        "There are several structures for each one-stage and two-stage  detector. R-CNN, Fast R-CNN, and Faster R-CNN are two-stage detectors, while YOLO, SSD, and RetinaNet are one-stage detectors. Let's look at the characteristics of each model structure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S05WCBBI6yb"
      },
      "source": [
        "### 1.3.1. R-CNN\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2qQ4LmjqSvJ"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1qCmgiqH45lkpzADBk3zh29RFPPrRVKC4)\n",
        "\n",
        "- 그림 1-8 R-CNN 구조 (출처: Girshick et al. 2014. Rich feature gierarchies for accurate object detection and semantic segmentation)\n",
        "\n",
        "R-CNN creates a region proposal for an image using Selective Search. Each created candidate region is wrapped in a fixed size by force and used as an input to the CNN. The feature map from the CNN is classified through SVM and the bounding-box is adjusted through Regressor. It has the disadvantage that it requires a large amount of storage space and is slow, since the image must be transformed or lost by wrapping and the CNN must be rotated as many times as the number of candidate regions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AQeHSVZI7Gv"
      },
      "source": [
        "### 1.3.2. Fast R-CNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6ChiKO_qXM8"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1zKLOeKylk2SjRMQmfJ3ZJdaJSs0q1fIw)\n",
        "\n",
        "- 그림 1-9 Fast R-CNN 구조 (출처: Girshick. ICCV 2015. Fast R-CNN)\n",
        "\n",
        "Unlike R-CNNs, which apply a CNN to each candidate region, in a Fast R-CNN structure, a candidate region is created from the feature map generated by applying a CNN to the entire image. The generated candidate region is extracted as a fixed size feature vector through RoI pooling. After passing through the FC layer on the feature vector, classify it through Softmax, and adjust the bounding-box through Regressor. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTc8a9gtI7dM"
      },
      "source": [
        "### 1.3.3. Faster R-CNN\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grq6yIZzqdcZ"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1O5sRVhjcVR8J8zFDhVNwWHmEo069j876) ![](https://drive.google.com/uc?export=view&id=18PW63VbzIdODeCRGSW0G9UF78Zmd5QY0)\n",
        "\n",
        "- 그림 1-10 Faster R-CNN 구조 (출처: Ren et al. 2016. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks)\n",
        "\n",
        "Faster R-CNNs use a Region Proposal Network (RPN), which uses deep learning in place of the Selective Search step. RPNs predict the candidate region with an anchor-box at each point taken by the sliding-window when calculating the CNN in the feature map. The anchor box is a bounding box with several preset ratios and sizes. The candidate regions obtained from the RPN are sorted in the order of IoU, and the final candidate regions are selected through the Non-Maximum Suppression (NMS) algorithm. To fix the size of the selected candidate region, RoI pooling is performed, and then the process proceeds in the same manner as a Fast R-CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMjtNQuaI7zY"
      },
      "source": [
        "### 1.3.4. YOLO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNGFbGaLqgbj"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1mVIx26ewRcNab82TGkFj1ODnvPSoQjoD)\n",
        "\n",
        "- 그림 1-11 YOLO 구조 (출처: Redmon et al. 2016. You Only Look Once: Unified, Real-Time Object Detection)\n",
        "\n",
        "Treating the bounding-box and class probability as a single problem, the YOLO structure predicts the class and location of an object at the same time. In order to use it, we divide the image into grids of a certain size to predict the bounding box for each grid. We will train the model with the confidence score values for the bounding boxes and the class score values for the grid cells. It is a very fast and simple process, but it is relatively inaccurate for small objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YOhpd4I8Gl"
      },
      "source": [
        "### 1.3.5. SSD\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8K0S7bsqjeI"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=11kZDrVy5qi4VIAPXfGLmaGV5l_5h68rO)\n",
        "\n",
        "- 그림 1-12 SSD 구조 (출처: Liu et al. 2016. SSD: Single Shot MultiBox Detector)\n",
        "\n",
        "The SSD calculates the bounding box's class score and offset (position coordinates) for each feature map that appears after each Convolutional Layer. The final bounding box is determined through the NMS algorithm. This has the advantage of being able to detect both small and large objects, since each feature map has a different scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GvYHJD8I8fq"
      },
      "source": [
        "### 1.3.6. RetinaNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PZ0-OvDqm8o"
      },
      "source": [
        "\n",
        "![](https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/pics/ch1img13.PNG?raw=true)\n",
        "\n",
        "- 그림 1-13 Focal Loss (출처: Lin et al. 2018. Focal Loss for Dense Object Detection)\n",
        "\n",
        "RetinaNet은 모델 학습시 계산하는 손실 함수(loss function)에 변화를 주어 기존 One-Stage Detector들이 지닌 낮은 성능을 개선했습니다. One-Stage Detector는 많게는 십만개 까지의 후보군 제시를 통해 학습을 진행합니다. 그 중 실제 객체인 것은 일반적으로 10개 이내 이고, 다수의 후보군이 background 클래스로 잡힙니다. 상대적으로 분류하기 쉬운 background 후보군들에 대한 loss값을 줄여줌으로써 분류하기 어려운 실제 객체들의 loss 비중을 높이고, 그에 따라 실제 객체들에 대한 학습에 집중하게 합니다. RetinaNet은 속도 빠르면서 Two-Stage Detector와 유사한 성능을 보입니다 "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [

      ],
      "name": "Ch1 Object Detection.ipynb",
      "provenance": [

      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
